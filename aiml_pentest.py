#!/usr/bin/env python3
"""
0xPrompt - LLM Exploitation Framework by d0sf3t

Usage:
    python aiml_pentest.py --help
    python aiml_pentest.py scan --target <API_URL> --type llm
    python aiml_pentest.py scan --target <API_URL> --type classifier
    python aiml_pentest.py supply-chain --path /path/to/project
    python aiml_pentest.py report --input results.json --output report.md
    python aiml_pentest.py corpus --target "User ID 02" --format json
    python aiml_pentest.py corpus -t "Admin credentials" -c prompt_injection jailbreak -p 5
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

# Framework imports
from scripts.utils.base import (
    setup_logging,
    APIModelInterface,
    LLMInterface,
    Finding,
    Severity,
    AttackCategory,
    TestResult
)

# Test modules
from scripts.adversarial.evasion_attacks import EvasionAttackModule
from scripts.model_extraction.extractor import ModelExtractionModule
from scripts.prompt_injection.injector import PromptInjectionModule
from scripts.data_poisoning.poisoning_tests import DataPoisoningModule
from scripts.supply_chain.scanner import SupplyChainScanner
from scripts.corpus_generator.generator import TestCorpusGenerator, TaxonomyCategory


class AIMLPentest:
    """Main orchestrator for AI/ML penetration testing"""

    def __init__(self, output_dir: Path, config: Optional[Dict] = None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.config = config or {}
        self.logger = setup_logging(logging.INFO)
        self.results: List[TestResult] = []
        self.findings: List[Finding] = []

    def run_full_assessment(
        self,
        target_url: str,
        target_type: str = "classifier",
        api_key: Optional[str] = None,
        modules: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Run comprehensive AI/ML security assessment

        Args:
            target_url: API endpoint for the target model
            target_type: Type of model (classifier, llm, regression)
            api_key: API key for authentication
            modules: List of modules to run (None = all applicable)

        Returns:
            Assessment results dictionary
        """
        self.logger.info(f"Starting AI/ML security assessment")
        self.logger.info(f"Target: {target_url}")
        self.logger.info(f"Type: {target_type}")

        start_time = datetime.now()

        # Initialize target interface
        if target_type == "llm":
            target = LLMInterface(
                endpoint=target_url,
                api_key=api_key,
                model_name=self.config.get('model_name', 'unknown'),
                rate_limit=self.config.get('rate_limit', 1.0)
            )
            applicable_modules = ['prompt_injection']
        else:
            target = APIModelInterface(
                endpoint=target_url,
                api_key=api_key,
                rate_limit=self.config.get('rate_limit', 1.0)
            )
            applicable_modules = ['evasion', 'extraction', 'poisoning']

        # Filter modules if specified
        if modules:
            applicable_modules = [m for m in modules if m in applicable_modules]

        # Run applicable test modules
        for module_name in applicable_modules:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"Running {module_name.upper()} tests")
            self.logger.info(f"{'='*60}")

            try:
                module = self._get_module(module_name, target)
                if module:
                    results = module.run_tests()
                    self.results.extend(results)
                    self.findings.extend(module.findings)
            except Exception as e:
                self.logger.error(f"Module {module_name} failed: {e}")

        # Generate summary
        duration = (datetime.now() - start_time).total_seconds()

        assessment = {
            "metadata": {
                "target": target_url,
                "target_type": target_type,
                "timestamp": start_time.isoformat(),
                "duration_seconds": duration,
                "modules_run": applicable_modules
            },
            "summary": self._generate_summary(),
            "findings": [f.to_dict() for f in self.findings],
            "results": [
                {
                    "test_name": r.test_name,
                    "success": r.success,
                    "attack_succeeded": r.attack_succeeded,
                    "metrics": r.metrics,
                    "duration": r.duration_seconds
                }
                for r in self.results
            ]
        }

        # Save results
        self._save_assessment(assessment)

        return assessment

    def run_supply_chain_scan(self, scan_path: Path) -> Dict[str, Any]:
        """Run supply chain security scan"""
        self.logger.info(f"Starting supply chain scan: {scan_path}")

        scanner = SupplyChainScanner(
            target=None,
            output_dir=self.output_dir / "supply_chain",
            config={"scan_path": str(scan_path)}
        )

        results = scanner.run_tests()
        self.results.extend(results)
        self.findings.extend(scanner.findings)

        return {
            "scan_path": str(scan_path),
            "findings": [f.to_dict() for f in scanner.findings],
            "results": [
                {
                    "test_name": r.test_name,
                    "attack_succeeded": r.attack_succeeded,
                    "metrics": r.metrics
                }
                for r in results
            ]
        }

    def generate_report(
        self,
        assessment: Dict[str, Any],
        output_path: Path,
        format: str = "markdown"
    ):
        """Generate assessment report"""
        self.logger.info(f"Generating {format} report: {output_path}")

        if format == "markdown":
            report = self._generate_markdown_report(assessment)
        elif format == "json":
            report = json.dumps(assessment, indent=2, default=str)
        elif format == "html":
            report = self._generate_html_report(assessment)
        else:
            raise ValueError(f"Unknown format: {format}")

        with open(output_path, 'w') as f:
            f.write(report)

        self.logger.info(f"Report saved to {output_path}")

    def _get_module(self, module_name: str, target):
        """Get test module instance"""
        module_config = self.config.get(module_name, {})
        module_output = self.output_dir / module_name

        modules = {
            'evasion': EvasionAttackModule,
            'extraction': ModelExtractionModule,
            'prompt_injection': PromptInjectionModule,
            'poisoning': DataPoisoningModule,
        }

        if module_name in modules:
            return modules[module_name](
                target=target,
                output_dir=module_output,
                config=module_config
            )
        return None

    def _generate_summary(self) -> Dict[str, Any]:
        """Generate assessment summary"""
        return {
            "total_tests": len(self.results),
            "successful_attacks": sum(1 for r in self.results if r.attack_succeeded),
            "total_findings": len(self.findings),
            "findings_by_severity": {
                sev.value: sum(1 for f in self.findings if f.severity == sev)
                for sev in Severity
            },
            "findings_by_category": {
                cat.value: sum(1 for f in self.findings if f.category == cat)
                for cat in AttackCategory
            },
            "risk_rating": self._calculate_risk_rating()
        }

    def _calculate_risk_rating(self) -> str:
        """Calculate overall risk rating"""
        critical = sum(1 for f in self.findings if f.severity == Severity.CRITICAL)
        high = sum(1 for f in self.findings if f.severity == Severity.HIGH)

        if critical >= 3 or (critical >= 1 and high >= 3):
            return "CRITICAL"
        elif critical >= 1 or high >= 3:
            return "HIGH"
        elif high >= 1:
            return "MEDIUM"
        elif self.findings:
            return "LOW"
        return "INFO"

    def _save_assessment(self, assessment: Dict[str, Any]):
        """Save assessment to file"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = self.output_dir / f"assessment_{timestamp}.json"

        with open(output_path, 'w') as f:
            json.dump(assessment, f, indent=2, default=str)

        self.logger.info(f"Assessment saved to {output_path}")

    def _generate_markdown_report(self, assessment: Dict[str, Any]) -> str:
        """Generate Markdown report"""
        summary = assessment.get('summary', {})
        findings = assessment.get('findings', [])
        metadata = assessment.get('metadata', {})

        report = f"""# AI/ML Security Assessment Report

## Executive Summary

| Metric | Value |
|--------|-------|
| **Target** | {metadata.get('target', 'N/A')} |
| **Assessment Date** | {metadata.get('timestamp', 'N/A')} |
| **Duration** | {metadata.get('duration_seconds', 0):.1f} seconds |
| **Overall Risk Rating** | **{summary.get('risk_rating', 'N/A')}** |
| **Total Findings** | {summary.get('total_findings', 0)} |
| **Successful Attacks** | {summary.get('successful_attacks', 0)} / {summary.get('total_tests', 0)} |

### Findings by Severity

| Severity | Count |
|----------|-------|
| Critical | {summary.get('findings_by_severity', {}).get('critical', 0)} |
| High | {summary.get('findings_by_severity', {}).get('high', 0)} |
| Medium | {summary.get('findings_by_severity', {}).get('medium', 0)} |
| Low | {summary.get('findings_by_severity', {}).get('low', 0)} |
| Info | {summary.get('findings_by_severity', {}).get('info', 0)} |

---

## Detailed Findings

"""
        # Sort findings by severity
        severity_order = ['critical', 'high', 'medium', 'low', 'info']
        sorted_findings = sorted(
            findings,
            key=lambda f: severity_order.index(f.get('severity', 'info'))
        )

        for i, finding in enumerate(sorted_findings, 1):
            severity_badge = {
                'critical': 'ðŸ”´ CRITICAL',
                'high': 'ðŸŸ  HIGH',
                'medium': 'ðŸŸ¡ MEDIUM',
                'low': 'ðŸŸ¢ LOW',
                'info': 'â„¹ï¸ INFO'
            }.get(finding.get('severity', 'info'), 'INFO')

            report += f"""### {i}. {finding.get('title', 'Untitled Finding')}

**Severity:** {severity_badge}
**Category:** {finding.get('category', 'N/A')}
**ID:** {finding.get('id', 'N/A')}

**Description:**
{finding.get('description', 'No description provided.')}

**Evidence:**
```json
{json.dumps(finding.get('evidence', {}), indent=2)[:500]}
```

**Remediation:**
{finding.get('remediation', 'No remediation guidance provided.')}

---

"""

        report += """## Test Results Summary

| Test | Status | Attack Success | Duration |
|------|--------|----------------|----------|
"""
        for result in assessment.get('results', []):
            status = "âœ…" if result.get('success') else "âŒ"
            attack = "âš ï¸ Yes" if result.get('attack_succeeded') else "âœ… No"
            duration = f"{result.get('duration', 0):.2f}s"
            report += f"| {result.get('test_name', 'Unknown')} | {status} | {attack} | {duration} |\n"

        report += """
---

## Recommendations

Based on the findings, the following actions are recommended:

1. **Immediate Actions (Critical/High)**
   - Address all critical and high severity findings immediately
   - Review and patch vulnerable dependencies
   - Implement input validation and output filtering

2. **Short-term Actions (Medium)**
   - Implement adversarial training
   - Add rate limiting and query monitoring
   - Review model access controls

3. **Long-term Actions (Low/Info)**
   - Establish ML security best practices
   - Implement continuous security testing
   - Train development team on ML security

---

*Report generated by AI/ML Pentesting Framework*
"""
        return report

    def _generate_html_report(self, assessment: Dict[str, Any]) -> str:
        """Generate HTML report"""
        # Convert markdown to basic HTML
        md_report = self._generate_markdown_report(assessment)

        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>AI/ML Security Assessment Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        h1 {{ color: #333; }}
        h2 {{ color: #666; border-bottom: 1px solid #ddd; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f4f4f4; }}
        pre {{ background-color: #f4f4f4; padding: 10px; overflow-x: auto; }}
        .critical {{ color: red; font-weight: bold; }}
        .high {{ color: orange; font-weight: bold; }}
        .medium {{ color: #cc0; font-weight: bold; }}
        .low {{ color: green; }}
    </style>
</head>
<body>
<pre style="white-space: pre-wrap;">
{md_report}
</pre>
</body>
</html>
"""
        return html


def main():
    parser = argparse.ArgumentParser(
        description="0xPrompt - LLM Exploitation Framework by d0sf3t",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Test an LLM API
    python aiml_pentest.py scan --target https://api.example.com/v1/chat --type llm --api-key $API_KEY

    # Test a classifier API
    python aiml_pentest.py scan --target https://api.example.com/predict --type classifier

    # Run supply chain scan
    python aiml_pentest.py supply-chain --path ./my-ml-project

    # Generate report from results
    python aiml_pentest.py report --input results.json --format markdown

    # Generate test corpus (generic)
    python aiml_pentest.py corpus --format json --output ./corpus

    # Generate corpus targeting specific data
    python aiml_pentest.py corpus --target "User ID 02" --format json

    # Generate corpus for specific categories with preview
    python aiml_pentest.py corpus -t "Admin credentials" -c prompt_injection jailbreak -p 5
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    # Scan command
    scan_parser = subparsers.add_parser('scan', help='Run security assessment against target')
    scan_parser.add_argument('--target', required=True, help='Target API endpoint URL')
    scan_parser.add_argument('--type', choices=['classifier', 'llm', 'regression'],
                            default='classifier', help='Target model type')
    scan_parser.add_argument('--api-key', help='API key for authentication')
    scan_parser.add_argument('--modules', nargs='+',
                            choices=['evasion', 'extraction', 'prompt_injection', 'poisoning'],
                            help='Specific modules to run')
    scan_parser.add_argument('--output', default='./aiml_pentest_results',
                            help='Output directory for results')
    scan_parser.add_argument('--config', help='Path to configuration file')
    scan_parser.add_argument('--rate-limit', type=float, default=1.0,
                            help='Queries per second rate limit')

    # Supply chain command
    supply_parser = subparsers.add_parser('supply-chain', help='Run supply chain security scan')
    supply_parser.add_argument('--path', required=True, help='Path to scan')
    supply_parser.add_argument('--output', default='./aiml_pentest_results',
                              help='Output directory for results')

    # Report command
    report_parser = subparsers.add_parser('report', help='Generate report from results')
    report_parser.add_argument('--input', required=True, help='Input assessment JSON file')
    report_parser.add_argument('--output', help='Output report file')
    report_parser.add_argument('--format', choices=['markdown', 'json', 'html'],
                              default='markdown', help='Report format')

    # Corpus generation command
    corpus_parser = subparsers.add_parser('corpus', help='Generate test corpus for AI/ML red teaming')
    corpus_parser.add_argument('--target', '-t', help='Target for payload interpolation (e.g., "User ID 02", "Admin credentials")')
    corpus_parser.add_argument('--categories', '-c', nargs='+',
                               choices=[cat.value for cat in TaxonomyCategory],
                               help='Specific categories to generate (default: all)')
    corpus_parser.add_argument('--output', '-o', default='./corpus_output',
                               help='Output directory for corpus files')
    corpus_parser.add_argument('--format', '-f', choices=['json', 'markdown', 'csv'],
                               default='json', help='Output format')
    corpus_parser.add_argument('--preview', '-p', type=int, default=0,
                               help='Preview N payloads to stdout (0 = no preview)')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    # Execute command
    if args.command == 'scan':
        config = {}
        if args.config:
            with open(args.config) as f:
                config = json.load(f)
        config['rate_limit'] = args.rate_limit

        framework = AIMLPentest(
            output_dir=Path(args.output),
            config=config
        )

        assessment = framework.run_full_assessment(
            target_url=args.target,
            target_type=args.type,
            api_key=args.api_key,
            modules=args.modules
        )

        # Generate report
        report_path = Path(args.output) / "report.md"
        framework.generate_report(assessment, report_path, "markdown")

        print(f"\n{'='*60}")
        print("Assessment Complete")
        print(f"{'='*60}")
        print(f"Risk Rating: {assessment['summary']['risk_rating']}")
        print(f"Total Findings: {assessment['summary']['total_findings']}")
        print(f"Results saved to: {args.output}")

    elif args.command == 'supply-chain':
        framework = AIMLPentest(output_dir=Path(args.output))
        results = framework.run_supply_chain_scan(Path(args.path))

        print(f"\n{'='*60}")
        print("Supply Chain Scan Complete")
        print(f"{'='*60}")
        print(f"Findings: {len(results['findings'])}")

    elif args.command == 'report':
        with open(args.input) as f:
            assessment = json.load(f)

        output_path = args.output or f"report.{args.format}"
        if args.format == 'markdown':
            output_path = output_path if output_path.endswith('.md') else f"{output_path}.md"
        elif args.format == 'html':
            output_path = output_path if output_path.endswith('.html') else f"{output_path}.html"

        framework = AIMLPentest(output_dir=Path('.'))
        framework.generate_report(assessment, Path(output_path), args.format)

    elif args.command == 'corpus':
        # Generate test corpus with optional target
        print(f"\n{'='*60}")
        print("Generating Test Corpus")
        print(f"{'='*60}")

        if args.target:
            print(f"Target: {args.target}")

        generator = TestCorpusGenerator(target=args.target)
        output = generator.generate_all()

        # Flatten test cases
        all_cases = []
        for cat_name, cases in output.categories.items():
            # Filter by requested categories if specified
            if args.categories and cat_name not in args.categories:
                continue
            all_cases.extend(cases)

        print(f"Generated {len(all_cases)} test cases")

        # Category breakdown
        category_counts = {}
        for tc in all_cases:
            cat = tc.category.value
            category_counts[cat] = category_counts.get(cat, 0) + 1

        print("\nCategories:")
        for cat, count in sorted(category_counts.items()):
            print(f"  - {cat}: {count}")

        # Preview payloads if requested
        if args.preview > 0:
            print(f"\n{'='*60}")
            print(f"Preview ({args.preview} payloads)")
            print(f"{'='*60}")
            for tc in all_cases[:args.preview]:
                print(f"\n[{tc.category.value}] {tc.name}")
                print(f"  Payload: {tc.payload[:200]}{'...' if len(tc.payload) > 200 else ''}")

        # Save output
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        target_suffix = f"_{args.target.replace(' ', '_')[:20]}" if args.target else ""

        if args.format == 'json':
            output_file = output_dir / f"corpus{target_suffix}_{timestamp}.json"
            corpus_data = {
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "target": args.target,
                    "total_count": len(all_cases),
                    "categories": category_counts
                },
                "test_cases": [
                    {
                        "id": tc.id,
                        "name": tc.name,
                        "category": tc.category.value,
                        "severity": tc.severity.value,
                        "payload": tc.payload,
                        "description": tc.description,
                        "expected_behavior": tc.expected_behavior,
                        "techniques_used": tc.techniques_used,
                        "complexity_level": tc.complexity_level
                    }
                    for tc in all_cases
                ]
            }
            with open(output_file, 'w') as f:
                json.dump(corpus_data, f, indent=2)

        elif args.format == 'markdown':
            output_file = output_dir / f"corpus{target_suffix}_{timestamp}.md"
            md_content = f"# 0xPrompt Test Corpus\n\n"
            if args.target:
                md_content += f"**Target:** {args.target}\n\n"
            md_content += f"**Generated:** {datetime.now().isoformat()}\n"
            md_content += f"**Total Test Cases:** {len(all_cases)}\n\n"
            md_content += "---\n\n"

            for tc in all_cases:
                md_content += f"## {tc.name}\n\n"
                md_content += f"- **ID:** {tc.id}\n"
                md_content += f"- **Category:** {tc.category.value}\n"
                md_content += f"- **Severity:** {tc.severity.value}\n"
                md_content += f"- **Complexity:** {tc.complexity_level}\n\n"
                md_content += f"**Description:** {tc.description}\n\n"
                md_content += f"**Payload:**\n```\n{tc.payload}\n```\n\n"
                md_content += "---\n\n"

            with open(output_file, 'w') as f:
                f.write(md_content)

        elif args.format == 'csv':
            import csv
            output_file = output_dir / f"corpus{target_suffix}_{timestamp}.csv"
            with open(output_file, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['ID', 'Name', 'Category', 'Severity', 'Complexity', 'Payload'])
                for tc in all_cases:
                    writer.writerow([tc.id, tc.name, tc.category.value, tc.severity.value,
                                     tc.complexity_level, tc.payload])

        print(f"\nCorpus saved to: {output_file}")


if __name__ == "__main__":
    main()
