# AI/ML Security Testing Methodology
## Comprehensive Penetration Testing Framework

**Version:** 1.0.0
**Date:** 16 February 2026
**Classification:** Authorized Security Testing Only

---

## Table of Contents

1. [Engagement Overview](#1-engagement-overview)
2. [Reconnaissance Phase](#2-reconnaissance-phase)
3. [Model Security Assessment](#3-model-security-assessment)
4. [Adversarial Attack Testing](#4-adversarial-attack-testing)
5. [Prompt Injection Testing](#5-prompt-injection-testing)
6. [Data Security Assessment](#6-data-security-assessment)
7. [Infrastructure Security](#7-infrastructure-security)
8. [Supply Chain Analysis](#8-supply-chain-analysis)
9. [Privacy & Inference Attacks](#9-privacy--inference-attacks)
10. [Reporting](#10-reporting)

---

## 1. Engagement Overview

### 1.1 Scope Definition

Before beginning any AI/ML security assessment, clearly define:

| Category | Details |
|----------|---------|
| **Target Systems** | Models, APIs, training pipelines, inference endpoints |
| **Authorization Level** | Black-box, gray-box, white-box |
| **Boundaries** | Production vs. staging, rate limits, data access |
| **Success Criteria** | What constitutes a finding |

### 1.2 AI/ML Attack Surface Map

```
┌─────────────────────────────────────────────────────────────────────┐
│                        AI/ML ATTACK SURFACE                         │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐          │
│  │   TRAINING   │───▶│    MODEL     │───▶│  INFERENCE   │          │
│  │   PIPELINE   │    │   STORAGE    │    │   ENDPOINT   │          │
│  └──────┬───────┘    └──────┬───────┘    └──────┬───────┘          │
│         │                   │                   │                   │
│         ▼                   ▼                   ▼                   │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐          │
│  │ Data Sources │    │Model Weights │    │ API Gateway  │          │
│  │ Annotations  │    │Architecture  │    │ Auth/AuthZ   │          │
│  │ Preprocessing│    │Hyperparams   │    │ Rate Limits  │          │
│  └──────────────┘    └──────────────┘    └──────────────┘          │
│                                                                     │
│  VECTORS: Poisoning│Extraction│Evasion│Injection│Inference│DoS     │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.3 Risk Categories (OWASP ML Top 10 + Extended)

| ID | Category | Severity | Description |
|----|----------|----------|-------------|
| ML01 | Input Manipulation | Critical | Adversarial examples, prompt injection |
| ML02 | Data Poisoning | Critical | Training data corruption |
| ML03 | Model Theft | High | Extraction and replication |
| ML04 | Model Inversion | High | Extracting training data |
| ML05 | Membership Inference | Medium | Determining training set membership |
| ML06 | Model Skewing | High | Manipulating model behavior |
| ML07 | Supply Chain | Critical | Compromised dependencies/models |
| ML08 | Output Manipulation | High | Forcing specific outputs |
| ML09 | Denial of Service | Medium | Resource exhaustion |
| ML10 | Privacy Leakage | High | PII exposure through outputs |

---

## 2. Reconnaissance Phase

### 2.1 Passive Reconnaissance Checklist

```
[ ] Identify ML framework (TensorFlow, PyTorch, JAX, etc.)
[ ] Enumerate API endpoints and documentation
[ ] Discover model cards and architecture details
[ ] Find training data sources
[ ] Identify model versioning systems
[ ] Map CI/CD pipelines
[ ] Review public GitHub repos for model artifacts
[ ] Search for leaked API keys and credentials
[ ] Analyze job postings for tech stack hints
[ ] Check HuggingFace, ModelZoo, Docker Hub for related models
```

### 2.2 Active Reconnaissance Checklist

```
[ ] Probe API rate limits and throttling
[ ] Identify input validation boundaries
[ ] Test authentication mechanisms
[ ] Enumerate available model endpoints
[ ] Discover hidden parameters
[ ] Identify model version through behavior differences
[ ] Test for verbose error messages
[ ] Map input/output dimensions
[ ] Identify preprocessing steps through probing
[ ] Detect model type (classification, regression, generative)
```

### 2.3 Fingerprinting Techniques

| Technique | Purpose | Script |
|-----------|---------|--------|
| Output Distribution Analysis | Identify model architecture | `scripts/recon/fingerprint_model.py` |
| Timing Analysis | Detect batch processing, caching | `scripts/recon/timing_analysis.py` |
| Error Message Harvesting | Reveal internal details | `scripts/recon/error_harvest.py` |
| Version Probing | Identify framework versions | `scripts/recon/version_probe.py` |

---

## 3. Model Security Assessment

### 3.1 Model Extraction Testing

**Objective:** Determine if model can be replicated through query access

#### Checkpoint: Pre-Extraction
```
[ ] Established query budget
[ ] Set up surrogate model architecture
[ ] Prepared training data generation strategy
[ ] Configured output logging
```

#### Extraction Methods

| Method | Complexity | Query Cost | Fidelity |
|--------|------------|------------|----------|
| Jacobian-based | Medium | High | High |
| Knockoff Networks | Low | Medium | Medium |
| PRADA | High | Low | High |
| Adaptive Query | Medium | Variable | High |

#### Checkpoint: Post-Extraction
```
[ ] Measured agreement rate with target model
[ ] Compared decision boundaries
[ ] Validated on holdout set
[ ] Documented extraction cost (queries, time)
```

### 3.2 Model Inversion Testing

**Objective:** Recover training data representations

```
[ ] Identify output confidence scores availability
[ ] Test gradient-based inversion (if gradients exposed)
[ ] Attempt feature reconstruction
[ ] Test with auxiliary information
[ ] Document recovered information fidelity
```

### 3.3 Membership Inference Testing

**Objective:** Determine if specific data was used in training

```
[ ] Train shadow models
[ ] Build attack classifier
[ ] Test with known members/non-members
[ ] Calculate attack accuracy
[ ] Assess privacy risk level
```

---

## 4. Adversarial Attack Testing

### 4.1 Evasion Attack Checklist

#### White-Box Attacks (Full Model Access)
```
[ ] Fast Gradient Sign Method (FGSM)
[ ] Projected Gradient Descent (PGD)
[ ] C&W Attack (Carlini & Wagner)
[ ] DeepFool
[ ] JSMA (Jacobian Saliency Map)
[ ] AutoAttack
```

#### Black-Box Attacks (Query Access Only)
```
[ ] Transfer Attacks (surrogate model)
[ ] Score-Based Attacks (ZOO, NES)
[ ] Decision-Based Attacks (Boundary, HopSkipJump)
[ ] Query-Efficient Attacks
```

#### Physical-World Attacks
```
[ ] Adversarial Patches
[ ] 3D Adversarial Objects
[ ] Environmental Perturbations
[ ] Printable Adversarial Examples
```

### 4.2 Attack Success Criteria

| Metric | Threshold | Description |
|--------|-----------|-------------|
| Attack Success Rate | >90% | Misclassification rate |
| Perturbation Budget | L∞ < 8/255 | Maximum pixel change |
| Query Budget | <10,000 | Queries to succeed |
| Transferability | >50% | Cross-model success |

### 4.3 Robustness Evaluation Checkpoint

```
[ ] Tested all applicable attack methods
[ ] Documented perturbation budgets
[ ] Measured natural accuracy vs. robust accuracy
[ ] Identified most effective attack vectors
[ ] Assessed defense bypass potential
[ ] Calculated attack cost-benefit ratio
```

---

## 5. Prompt Injection Testing

### 5.1 LLM-Specific Attack Surface

```
┌─────────────────────────────────────────────────────────────────┐
│                    LLM ATTACK TAXONOMY                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  DIRECT INJECTION          INDIRECT INJECTION                  │
│  ┌─────────────────┐       ┌─────────────────┐                 │
│  │ User Input      │       │ External Sources│                 │
│  │ System Prompt   │       │ RAG Documents   │                 │
│  │ Few-shot Poison │       │ Tool Outputs    │                 │
│  └─────────────────┘       └─────────────────┘                 │
│                                                                 │
│  PAYLOAD TYPES                                                  │
│  ├── Jailbreaks (safety bypass)                                │
│  ├── Prompt Leaking (extract system prompt)                    │
│  ├── Goal Hijacking (redirect behavior)                        │
│  ├── Data Exfiltration (steal context)                         │
│  └── Denial of Service (resource exhaustion)                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 Direct Injection Checklist

```
[ ] Basic instruction override
[ ] Role-playing attacks (DAN, etc.)
[ ] Language switching attacks
[ ] Encoding attacks (base64, rot13)
[ ] Token manipulation
[ ] Delimiter confusion
[ ] Markdown/formatting injection
[ ] Multi-turn conversation attacks
[ ] Context window overflow
```

### 5.3 Indirect Injection Checklist

```
[ ] RAG poisoning (inject into knowledge base)
[ ] Tool output injection
[ ] Web scraping injection
[ ] Email/document injection
[ ] Image-based injection (multimodal)
[ ] API response injection
[ ] Database injection (vector DBs)
```

### 5.4 Jailbreak Categories

| Category | Description | Example Technique |
|----------|-------------|-------------------|
| Roleplay | Assume unrestricted persona | "You are DAN..." |
| Hypothetical | Frame as fiction | "In a story where..." |
| Gradual | Incremental escalation | Start benign, escalate |
| Encoding | Obfuscate payload | Base64, ROT13, leetspeak |
| Multi-language | Language confusion | Mix languages |
| Token Smuggling | Exploit tokenization | Special characters |

### 5.5 Prompt Injection Testing Checkpoint

```
[ ] Tested all direct injection categories
[ ] Tested indirect injection vectors
[ ] Documented successful bypasses
[ ] Measured injection reliability
[ ] Identified defense mechanisms
[ ] Tested defense bypass techniques
[ ] Documented payload library used
```

---

## 6. Data Security Assessment

### 6.1 Training Data Poisoning

#### Attack Vectors
```
[ ] Label Flipping (change labels)
[ ] Backdoor Injection (trigger patterns)
[ ] Clean-Label Attacks (no label change)
[ ] Feature Collision Attacks
[ ] Gradient-Based Poisoning
```

#### Checkpoint: Poisoning Feasibility
```
[ ] Assessed data pipeline access
[ ] Identified injection points
[ ] Calculated required poison rate
[ ] Tested trigger persistence
[ ] Validated attack transferability
```

### 6.2 Data Leakage Assessment

```
[ ] Test for training data memorization
[ ] Probe for PII in outputs
[ ] Test extraction of sensitive patterns
[ ] Assess differential privacy implementation
[ ] Check for data in error messages
[ ] Validate data anonymization
```

### 6.3 Federated Learning Attacks

```
[ ] Model poisoning attacks
[ ] Gradient inference attacks
[ ] Free-rider detection
[ ] Sybil attack testing
[ ] Byzantine fault testing
```

---

## 7. Infrastructure Security

### 7.1 API Security Checklist

```
[ ] Authentication strength
[ ] Authorization granularity
[ ] Rate limiting effectiveness
[ ] Input validation coverage
[ ] Output sanitization
[ ] Error handling security
[ ] Logging completeness
[ ] TLS configuration
[ ] API key rotation policy
[ ] Abuse detection mechanisms
```

### 7.2 Model Serving Infrastructure

```
[ ] Container security
[ ] Kubernetes security (if applicable)
[ ] Model storage security
[ ] Secrets management
[ ] Network segmentation
[ ] Monitoring and alerting
[ ] Incident response procedures
```

### 7.3 GPU/TPU Infrastructure

```
[ ] Shared memory attacks
[ ] Side-channel vulnerabilities
[ ] Resource isolation
[ ] Multi-tenancy security
```

---

## 8. Supply Chain Analysis

### 8.1 Dependency Assessment

```
[ ] Scan ML framework vulnerabilities
[ ] Check pre-trained model provenance
[ ] Validate dataset sources
[ ] Audit third-party code
[ ] Review container base images
[ ] Check for typosquatting
```

### 8.2 Pre-trained Model Risks

```
[ ] Backdoor detection in weights
[ ] Architecture verification
[ ] Training data validation
[ ] License compliance
[ ] Model card accuracy
```

### 8.3 Supply Chain Checkpoint

```
[ ] Created dependency SBOM
[ ] Scanned for known CVEs
[ ] Validated model signatures
[ ] Reviewed model provenance
[ ] Tested for embedded backdoors
[ ] Documented supply chain risks
```

---

## 9. Privacy & Inference Attacks

### 9.1 Privacy Assessment Matrix

| Attack | Risk | Mitigation |
|--------|------|------------|
| Membership Inference | Training data exposure | Differential privacy |
| Model Inversion | Feature reconstruction | Output perturbation |
| Attribute Inference | Sensitive attribute exposure | Access controls |
| Property Inference | Training data properties | Federated learning |

### 9.2 Differential Privacy Testing

```
[ ] Verify ε (epsilon) budget
[ ] Test privacy guarantees
[ ] Assess utility-privacy tradeoff
[ ] Check for implementation flaws
```

### 9.3 Privacy Testing Checkpoint

```
[ ] Completed membership inference tests
[ ] Completed model inversion tests
[ ] Assessed attribute inference risk
[ ] Documented privacy budget
[ ] Evaluated anonymization effectiveness
```

---

## 10. Reporting

### 10.1 Finding Classification

| Severity | CVSS Equivalent | Example |
|----------|-----------------|---------|
| Critical | 9.0-10.0 | Remote model takeover, full extraction |
| High | 7.0-8.9 | Training data exposure, reliable jailbreak |
| Medium | 4.0-6.9 | Partial extraction, membership inference |
| Low | 0.1-3.9 | Information disclosure, DoS |
| Info | N/A | Best practice recommendations |

### 10.2 Report Sections

1. **Executive Summary**
2. **Scope and Methodology**
3. **Findings Summary**
4. **Detailed Findings**
5. **Risk Assessment**
6. **Remediation Recommendations**
7. **Appendices (PoCs, logs, evidence)**

### 10.3 Final Checkpoint

```
[ ] All phases completed
[ ] All findings documented
[ ] PoCs created and validated
[ ] Risk ratings assigned
[ ] Remediation guidance provided
[ ] Report reviewed for accuracy
[ ] Engagement artifacts secured
[ ] Client debrief scheduled
```

---

## Appendix A: Tool References

| Category | Tools |
|----------|-------|
| Adversarial | ART, Foolbox, CleverHans, Adversarial Robustness Toolbox |
| Extraction | Knockoff Nets, PRADA |
| Privacy | ML Privacy Meter, TensorFlow Privacy |
| Prompt Injection | Garak, custom framework |
| Fuzzing | AFL-ML, TensorFuzz |
| Scanning | Modelscan, Fickling |

## Appendix B: Reference Standards

- OWASP ML Security Top 10
- MITRE ATLAS
- NIST AI Risk Management Framework
- EU AI Act Requirements
- ISO/IEC 23894

---

*This methodology should be adapted based on specific engagement requirements and authorization scope.*
